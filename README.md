# ğŸ§  AI Subjective Answer Evaluation System

An AI-based system that automates evaluation of handwritten or typed subjective answers. It extracts text from scanned PDFs/images, compares it with model answers using NLP and semantic similarity, and scores responses in real time â€” ensuring faster, fairer, and more consistent grading.

## ğŸ¯ Goal

To build an end-to-end system that:
- Accepts scanned answer sheets (PDF/images)
- Extracts answers using OCR
- Compares answers with model responses
- Scores them based on relevance, clarity, and structure
- Displays results via a web dashboard or API

## ğŸš€ Features & Workflow

- ğŸ“„ Upload scanned PDFs or handwritten answer images
- ğŸ‘ Extracts handwritten text using **TrOCR** with OpenCV-based preprocessing
- ğŸ§¬ Generates or accepts model answers (via **GPT**, **T5**, or manual input)
- ğŸ” Scores student answers using **semantic similarity** (BERT/SentenceTransformers)
- ğŸ–¼ Detects and analyzes diagrams using **OpenCV**
- âš™ Processes all answersheets through a modular **FastAPI** backend
- ğŸ“Š Displays real-time feedback (WIP) with per-question analytics using **Streamlit**
- ğŸ§± Supports end-to-end **PDF processing pipeline**
- ğŸ”„ Modular design for easy extension or integration into other systems


## ğŸ§° Tech Stack

- **Languages & Frameworks**: Python, FastAPI, Streamlit
- **OCR & Image Processing**: TrOCR, Tesseract OCR, OpenCV, pdf2image, Pillow
- **NLP & AI Models**: Hugging Face Transformers, SentenceTransformers


## âœ… Current Progress

- [x] PDF/Image upload + segmentation
- [x] OCR extraction (typed + handwritten)
- [x] Text cleaning and diagram-aware zoning
- [ ] Semantic embedding + scoring logic
- [ ] Streamlit dashboard and API endpoints

## ğŸ— Architecture

Upload â†’ Convert â†’ OCR â†’ Clean â†’ Embed â†’ Compare â†’ Score â†’ View



